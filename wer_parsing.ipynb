{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "### Обязательная часть\n",
    "\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "В итоге должен формироваться датафрейм вида: <дата> - <заголовок> - <ссылка>\n",
    "\n",
    "### Дополнительная часть (необязательная)\n",
    "Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст_статьи>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём функцию, которая собирает с сайта habr.com/ru ссылки на все посты материалы (статьи, посты),\n",
    "# в которых содержится указанное ключевое слово\n",
    "\n",
    "def get_all_links(query, pages):\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    for page in range(1, pages+1):\n",
    "        \n",
    "        URL = 'https://habr.com/ru/search/' + 'page' + str(page)\n",
    "        params = {\n",
    "        'q': str(query)\n",
    "        }\n",
    "\n",
    "        res = requests.get(URL, params)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        articles = soup.find_all('article')\n",
    "        list_of_links = list(map(lambda x: x.find('a', class_='post__title_link').get('href'), articles))\n",
    "        links.extend(list_of_links)\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Применяем данную функцию для ключевых слов из списка keywords\n",
    "\n",
    "keywords = ['python', 'парсинг']\n",
    "\n",
    "links = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    links.extend(get_all_links(keyword, 50))\n",
    "    \n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# и теперь в цикле для каждой ссылки из списка извлекаем и добавляем в датафрейм искомую информацию:\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for link in links:\n",
    "    \n",
    "    request = requests.get(link)\n",
    "    link_soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    \n",
    "    # опытным путём обнаружено, что дата и заголовок хранятся в словаре, который можно спасрсить следующим образом:    \n",
    "    \n",
    "    article_dict = json.loads(''.join(link_soup.find('script', {'type': 'application/ld+json'}).contents))\n",
    "    \n",
    "    date = datetime.strptime(article_dict['datePublished'], '%Y-%m-%dT%H:%M:%S%z')\n",
    "    headline = article_dict['headline']\n",
    "    \n",
    "    # опытным путём обнаружено, что текст статьи на страницах размечен по-разному, поэтому его парсим по-разному:\n",
    "    \n",
    "    text = []\n",
    "    \n",
    "    if link_soup.find('div', class_ = 'post__text post__text_v1'):\n",
    "        text = link_soup.find('div', class_ = 'post__text post__text_v1').text\n",
    "    \n",
    "    if link_soup.find('div', class_ = 'post__text post__text_v2'):\n",
    "        text = link_soup.find('div', class_ = 'post__text post__text_v2').text\n",
    "        \n",
    "    if link_soup.find('div', class_ = 'post__text post__text-html post__text_v1'):\n",
    "        text = link_soup.find('div', class_ = 'post__text post__text-html post__text_v1').text\n",
    "        \n",
    "    row = {'post_date': date, \n",
    "           'headline': headline, \n",
    "           'link': link, \n",
    "           'text': text }\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame([row])])\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "Обязательная часть\n",
    "\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <почта> - <дата утечки> - <источник утечки> - <описание утечки>\n",
    "\n",
    "Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = ['nmmmm@xaker.ru', 'sa@mail.ru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leaks = pd.DataFrame()\n",
    "\n",
    "for email in emails:\n",
    "    \n",
    "    URL = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "\n",
    "    params = { 'emailAddresses': [email] }\n",
    "    \n",
    "    headers = {'Vaar-Version': '0',\n",
    "               'Vaar-Header-App-Product': 'hackcheck-web-avast',\n",
    "               'Content-Type': 'application/json;charset=UTF-8',\n",
    "               'Host': 'identityprotection.avast.com',\n",
    "               'Origin': 'https://www.avast.com',\n",
    "               'Referer': 'https://www.avast.com/',\n",
    "               'vaar-header-captcha-response-token': '03AGdBq26r4LzketNJyAZwqhNn_xUvpCDVBWeqSi22LGFuHGtYXmmvuMaM0SyDB5z4ZMkSpXguHvoTssuWm_FrQeB9gaNmGcIupugGIoz1c2eJbMJXHfXnG-BS6XnldZBVWX4MkyGsDkc0wPsgjbqy8VwlBphCqP87nEbtYiXUTa6NQbpMJYlgGyHMf5sXNMvPAiXhm1SnDkQbSY9uBZ_34VSxUYgKNJJ6C65kc-qnnxWa7qJr5rcy0ofdR9tNLWQWqxiZG49joN0o3QYHjqR76XrM-SpyYue2UhILbYAKdVGg5SrwBhYWd5sPXepeV64eChO8vQc6WeDyriLBUAHvhpCuGD1yCabCtRBtYZnMur0NqGZ9pLYFlWJR8x_DwUfYPv6ARLpqM74gwjYj75mdTVjv195wSDhL9xUWKv7CsBmqaKg_MezbtXTVkSPjoSz0cu_z1L6GA4Vg' }\n",
    "\n",
    "    req = requests.post(URL, json=params, headers=headers)\n",
    "    response_soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    response_json = json.loads(str(response_soup))\n",
    "    \n",
    "    for breachID in response_json['summary'][str(email)]['breaches']:\n",
    "        \n",
    "        e_mail = email\n",
    "        date = datetime.strptime(response_json['breaches'][str(breachID)]['publishDate'], '%Y-%m-%dT%H:%M:%S%z')\n",
    "        website = response_json['breaches'][str(breachID)]['site']\n",
    "        comment = response_json['breaches'][str(breachID)]['description']\n",
    "\n",
    "        row = {'email': e_mail,\n",
    "               'date': date,\n",
    "               'website': website,\n",
    "               'comment': comment }\n",
    "\n",
    "        df_leaks = pd.concat([df_leaks, pd.DataFrame([row])])\n",
    "\n",
    "df_leaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
